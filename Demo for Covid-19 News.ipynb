{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os, time, datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News in the website www.intopic.it have a publishing date which is written in italian using a non-standard format. Use the following function to convert it into a datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(filename, raw_string):\n",
    "    raw_string = str(raw_string)\n",
    "    months = {'Gennaio':'01', 'Febbraio':'02', 'Marzo':'03', 'Aprile':'04', 'Maggio':'05'}\n",
    "\n",
    "    if \"2020\" in raw_string:\n",
    "        creation_date = raw_string.split(' ')\n",
    "        form_date = str(creation_date[3]) +'-'+ str(months[creation_date[2]]) + '-' + str(creation_date[1])\n",
    "        #print(form_date)\n",
    "    else:\n",
    "        #print(raw_string)\n",
    "        creation_date = time.ctime(os.path.getctime(filename))\n",
    "        form_date = str(datetime.datetime.strptime(creation_date, \"%a %b %d %H:%M:%S %Y\"))\n",
    "        form_date = form_date.split(' ')[0]\n",
    "        #print(form_date)\n",
    "    \n",
    "    return form_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start with the code for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "html_root = \"/home/marco/workspace/git/StatLearnTeam/web_pages_index/\" # Where the html pages are\n",
    "\n",
    "cols = ['title', 'content', 'date', 'author', 'tags']\n",
    "articles_df = pd.DataFrame(data = None, columns = cols)\n",
    "\n",
    "\n",
    "\n",
    "for i in (list(range(1, 2685))[::-1]): # From last page to most recent\n",
    "    \n",
    "    webpage_path = html_root + str(i) + \".html\"\n",
    "    html_content = open(webpage_path)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser') # Open file as a webpage\n",
    "    \n",
    "    ################# SINGLE PAGE MINING STARTS HERE #############################\n",
    "    \n",
    "    article_section = soup.findAll('div', attrs={\"class\":\"bp-entry\"})\n",
    "    for article in article_section:\n",
    "        try:\n",
    "            title = article.find(\"h2\").find(\"a\").getText()\n",
    "            content = article.find(\"div\", attrs = {\"class\":\"bp-details\"}).getText()\n",
    "            publication_info = article.find(\"span\", attrs = {\"class\":\"author vcard\"}) # Date, author...\n",
    "            \n",
    "            date = extract_date(webpage_path, str(publication_info.getText()))\n",
    "            \n",
    "            author = publication_info.find(\"span\", attrs={\"class\":\"fn\"}).getText()\n",
    "            \n",
    "            raw_proxy_link = article.find(\"div\", attrs = {\"class\":\"bp-\"})\n",
    "            \n",
    "            # Not all articles have tags, but should not be a problem getting the other info (always useful)\n",
    "            # That's why there is a nested try except.\n",
    "            tags = list()\n",
    "            try:\n",
    "                tags_raw = article.find(\"div\", {\"class\":\"tagcloud\"}).findAll(\"a\",{'class':'tag-link-10'})\n",
    "                tags.extend([tag.getText() for tag in tags_raw])\n",
    "            except:\n",
    "                pass # Not an article, OR the article does not have any tags\n",
    "            \n",
    "            article_entry = pd.DataFrame(data = [[title, content, date, author, tags]], columns = cols)\n",
    "            articles_df = articles_df.append(article_entry, ignore_index = True)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass # Not all html element retrieved are actually article so exceptions could be thrown.\n",
    "            #print(e)\n",
    "            #print(\"=\" * 10)\n",
    "            #print(\"\\nNOT AN ARTICLE!\\n\")\n",
    "            #print(article)\n",
    "            #print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's preview the dataset we have generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shall now get some insight on the content of the articles. Let's look up some virus-related keywords and see if they appear in the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DATAFRAME ANALYSIS STARTS HERE ######################    \n",
    "contains_virus_count = {}\n",
    "\n",
    "for i in range(0, len(articles_df)):\n",
    "    row = articles_df.values[i]\n",
    "    \n",
    "    title = row[0]\n",
    "    content = row[1]\n",
    "    date = row[2]\n",
    "    author = row[3]\n",
    "    tags = row[4]\n",
    "    \n",
    "    aggregate_fields = [title, content]\n",
    "    aggregate_fields.extend(tags)\n",
    "    \n",
    "    if date not in contains_virus_count.keys():\n",
    "        contains_virus_count[date] = 0\n",
    "\n",
    "    keywords = ['coronavirus', 'covid', 'covid-19']            \n",
    "    has_keyword = False # Until proven true\n",
    "    \n",
    "    for field in aggregate_fields:\n",
    "        \n",
    "        if any(k in str.lower(field) for k in keywords):\n",
    "            has_keyword = True\n",
    "            \n",
    "    if has_keyword:\n",
    "        contains_virus_count[date] = contains_virus_count[date] + 1  # 1 More article contains coronavirus related keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_virus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a day by day count of articles that have the chosen keywords just in the title or content preview. \n",
    "Now the\n",
    "# plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.xticks(rotation=90)       \n",
    "\n",
    "dates = list(contains_virus_count.keys())\n",
    "num_of_articles = list(contains_virus_count.values())\n",
    "\n",
    "sum_articles = 0\n",
    "total_num_of_articles = []\n",
    "for v in num_of_articles:\n",
    "    sum_articles += v\n",
    "    total_num_of_articles.append(sum_articles)\n",
    "\n",
    "\n",
    "plt.title(\"Day by day count - Non cumulative\")\n",
    "plt.xlabel(\"Day of the month\")\n",
    "plt.ylabel(\"# of new articles\")\n",
    "\n",
    "plt.scatter(dates, num_of_articles, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Total number of articles available\")\n",
    "plt.title('Day by day count - Cumulative')\n",
    "plt.scatter(dates, total_num_of_articles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of articles by news outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_articles = articles_df.groupby('author').count()\n",
    "\n",
    "outlets_news_numbers = dict(zip(counter_articles.index.values, counter_articles['title'].values))\n",
    "\n",
    "outlets_news_numbers = collections.OrderedDict(sorted(outlets_news_numbers.items(), key = lambda x : x[1], reverse = True))\n",
    "\n",
    "print('There are ', len(outlets_news_numbers.keys()), ' news outlets. \\n')\n",
    "\n",
    "for key in outlets_news_numbers.keys():\n",
    "    print(key + ' : ', outlets_news_numbers[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
